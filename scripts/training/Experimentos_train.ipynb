{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numbers import Number\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import zipfile\n",
    "from google.colab import files\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from google.colab import drive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación tenemos el proceso de selección de las categorías para el proceso de modelamiento, y la reasignación de las etiquetas sin perder el conocimiento de las etiquetas originales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [51,88,77,76,73,46,81,17,41,89,74,94,58,43,60,95,82,83,80,23,75,78,18,50] # categorías del conjunto de validación con al menos 10 imagenes\n",
    "b = [73,77,51,46,82,43,94,89,58,30,90,74,83,41,95,78,53,81,75,88,80,57,92,72,44] # categorías del conjunto de validación con al menos 10 imagens\n",
    "c = np.intersect1d(a, b).tolist()\n",
    "\n",
    "ordered_to_id = {i: j for i, j in enumerate(c)}\n",
    "id_to_ordered = {j: i for i, j in ordered_to_id.items()}\n",
    "def convert_id(value, via):\n",
    "  if via == 'to_id':\n",
    "    return ordered_to_id[value]\n",
    "  if via == 'to_ordered':\n",
    "    return id_to_ordered[value]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocesamiento de los datos para poder aplicar los experimientos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train_1 = tf.keras.utils.to_categorical(Y_train_correct)\n",
    "Y_val_1 = tf.keras.utils.to_categorical(Y_valid_correct)\n",
    "Y_test_1 = tf.keras.utils.to_categorical(Y_test_correct)\n",
    "\n",
    "train_datagen_1 = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=90,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='constant'\n",
    ")\n",
    "train_gen_1 = train_datagen_1.flow(x=X_train,\n",
    "                               y=Y_train_1,\n",
    "                               batch_size=32)\n",
    "\n",
    "valid_datagen_1 = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    rescale=1./255\n",
    ")\n",
    "valid_gen_1 = valid_datagen_1.flow(x=X_valid,\n",
    "                               y=Y_val_1,\n",
    "                               batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primer experimento, el cual se define por la capa de entrada para imágenes 224x224, seis bloques de capas convolucionales (una capa convolucional y una capa de pooling), capa de vectorización de las matrices de las capas convolucionales, dos capas densas de 128 neuronas cada una, un dropout y la capa de salida para la clasificación de 18 categorías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos el modelo en keras\n",
    "conv_net = tf.keras.models.Sequential()\n",
    "\n",
    "#Capa de entrada\n",
    "width  =X_train.shape[1]\n",
    "height  =X_train.shape[2]\n",
    "conv_net.add(tf.keras.layers.Input(shape=(width, height, 3)))\n",
    "\n",
    "#Primer bloque\n",
    "conv_net.add(tf.keras.layers.Conv2D(\n",
    "    filters = 8, kernel_size = 5, activation = 'relu'\n",
    "))\n",
    "conv_net.add(tf.keras.layers.MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "#Segundo bloque\n",
    "conv_net.add(tf.keras.layers.Conv2D(\n",
    "    filters = 16,\n",
    "    kernel_size = 4,\n",
    "    activation = 'relu'\n",
    "))\n",
    "conv_net.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#Tercer bloque\n",
    "conv_net.add(tf.keras.layers.Conv2D(\n",
    "    filters = 32,\n",
    "    kernel_size = 3,\n",
    "    activation = 'relu'\n",
    "))\n",
    "conv_net.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#Cuarto bloque\n",
    "conv_net.add(tf.keras.layers.Conv2D(\n",
    "    filters = 64,\n",
    "    kernel_size = 3,\n",
    "    activation = 'relu'\n",
    "))\n",
    "conv_net.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#Quinto bloque\n",
    "conv_net.add(tf.keras.layers.Conv2D(\n",
    "    filters = 128,\n",
    "    kernel_size = 3,\n",
    "    activation = 'relu'\n",
    "))\n",
    "conv_net.add(tf.keras.layers.MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "#Sexto bloque\n",
    "conv_net.add(tf.keras.layers.Conv2D(\n",
    "    filters = 256,\n",
    "    kernel_size = 2,\n",
    "    activation = 'relu'\n",
    "))\n",
    "conv_net.add(tf.keras.layers.MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "#capa flatten, que transforma cualquier arreglo multidimensional en un vector unidimensional.\n",
    "conv_net.add(tf.keras.layers.Flatten())\n",
    "\n",
    "#####Clasificador\n",
    "# Capa densa intermedia\n",
    "conv_net.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
    "\n",
    "# Capa densa intermedia\n",
    "conv_net.add(tf.keras.layers.Dense(units=128, activation='relu'))\n",
    "\n",
    "# Agregamos dropout para regularización\n",
    "conv_net.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "# Capa de salida\n",
    "conv_net.add(tf.keras.layers.Dense(units=18, #Número de categorías\n",
    "                                   activation='softmax')) #Clasificación multiclase\n",
    "conv_net.summary()\n",
    "tf.keras.utils.plot_model(conv_net,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_net.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=tf.optimizers.Adam(learning_rate=1e-5),\n",
    "                 metrics = [\"accuracy\"])\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "                  filepath='best_weights_128N.h5', #################################################\n",
    "                  monitor=\"val_loss\",\n",
    "                  mode=\"min\",\n",
    "                  save_best_only=True,\n",
    "                  save_weights_only=True\n",
    "              )\n",
    "\n",
    "stopping = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_loss\",\n",
    "                patience=10,\n",
    "                mode=\"min\",\n",
    "                restore_best_weights=True\n",
    "            )\n",
    "\n",
    "\n",
    "total_samples = X_train.shape[0]\n",
    "validation_samples= X_valid.shape[0]\n",
    "batch_size = 32\n",
    "\n",
    "# Entrenamos el modelo\n",
    "history = conv_net.fit(train_gen_1,\n",
    "                       steps_per_epoch=total_samples // batch_size,\n",
    "                       epochs=50,\n",
    "                       validation_data= valid_gen_1 ,\n",
    "                       #validation_steps=validation_samples // batch_size,\n",
    "                       #verbose=1,\n",
    "                       callbacks=[checkpoint, stopping] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segundo experimento, el cual se define por la capa de entrada para imágenes 224x224, cinco bloques de capas convolucionales (una capa convolucional y una capa de pooling), capa de vectorización de las matrices de las capas convolucionales, dos capas densas de 256 neuronas cada una, un dropout y la capa de salida para la clasificación de 18 categorías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos el modelo en keras\n",
    "conv_net = tf.keras.models.Sequential()\n",
    "\n",
    "#Capa de entrada\n",
    "width  =X_train.shape[1]\n",
    "height  =X_train.shape[2]\n",
    "conv_net.add(tf.keras.layers.Input(shape=(width, height, 3)))\n",
    "\n",
    "#Primer bloque\n",
    "conv_net.add(tf.keras.layers.Conv2D(\n",
    "    filters = 8, kernel_size = 5, activation = 'relu'\n",
    "))\n",
    "conv_net.add(tf.keras.layers.MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "#Segundo bloque\n",
    "conv_net.add(tf.keras.layers.Conv2D(\n",
    "    filters = 16,\n",
    "    kernel_size = 4,\n",
    "    activation = 'relu'\n",
    "))\n",
    "conv_net.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#Tercer bloque\n",
    "conv_net.add(tf.keras.layers.Conv2D(\n",
    "    filters = 32,\n",
    "    kernel_size = 3,\n",
    "    activation = 'relu'\n",
    "))\n",
    "conv_net.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#Cuarto bloque\n",
    "conv_net.add(tf.keras.layers.Conv2D(\n",
    "    filters = 64,\n",
    "    kernel_size = 3,\n",
    "    activation = 'relu'\n",
    "))\n",
    "conv_net.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#Quinto bloque\n",
    "conv_net.add(tf.keras.layers.Conv2D(\n",
    "    filters = 128,\n",
    "    kernel_size = 3,\n",
    "    activation = 'relu'\n",
    "))\n",
    "conv_net.add(tf.keras.layers.MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "#capa flatten, que transforma cualquier arreglo multidimensional en un vector unidimensional.\n",
    "conv_net.add(tf.keras.layers.Flatten())\n",
    "\n",
    "#####Clasificador\n",
    "# Capa densa intermedia\n",
    "conv_net.add(tf.keras.layers.Dense(units=256, activation='relu'))\n",
    "\n",
    "# Capa densa intermedia\n",
    "conv_net.add(tf.keras.layers.Dense(units=256, activation='relu'))\n",
    "\n",
    "# Agregamos dropout para regularización\n",
    "conv_net.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "# Capa de salida\n",
    "conv_net.add(tf.keras.layers.Dense(units=18, #Número de categorías\n",
    "                                   activation='softmax')) #Clasificación multiclase\n",
    "\n",
    "conv_net.summary()\n",
    "tf.keras.utils.plot_model(conv_net,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_net.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=tf.optimizers.Adam(learning_rate=1e-5),\n",
    "                 metrics = [\"accuracy\"])\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "                  filepath='best_weights_256N.h5',\n",
    "                  monitor=\"val_loss\",\n",
    "                  mode=\"min\",\n",
    "                  save_best_only=True,\n",
    "                  save_weights_only=True\n",
    "              )\n",
    "\n",
    "stopping = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_loss\",\n",
    "                patience=10,\n",
    "                mode=\"min\",\n",
    "                restore_best_weights=True\n",
    "            )\n",
    "\n",
    "\n",
    "total_samples = X_train.shape[0]\n",
    "validation_samples= X_valid.shape[0]\n",
    "batch_size = 32\n",
    "\n",
    "# Entrenamos el modelo\n",
    "history = conv_net.fit(train_gen_1,\n",
    "                       steps_per_epoch=total_samples // batch_size,\n",
    "                       epochs=50,\n",
    "                       validation_data= valid_gen_1 ,\n",
    "                       #validation_steps=validation_samples // batch_size,\n",
    "                       #verbose=1,\n",
    "                       callbacks=[checkpoint, stopping] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tercer experimento, el cual se define por la capa de entrada para imágenes 224x224, cuatro bloques de capas convolucionales (una capa convolucional y una capa de pooling), capa de vectorización de las matrices de las capas convolucionales, dos capas densas de 512 neuronas cada una, un dropout y la capa de salida para la clasificación de 18 categorías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definimos el modelo en keras\n",
    "conv_net = tf.keras.models.Sequential()\n",
    "\n",
    "#Capa de entrada\n",
    "width  =X_train.shape[1]\n",
    "height  =X_train.shape[2]\n",
    "conv_net.add(tf.keras.layers.Input(shape=(width, height, 3)))\n",
    "\n",
    "#Primer bloque\n",
    "conv_net.add(tf.keras.layers.Conv2D(\n",
    "    filters = 8, kernel_size = 6, activation = 'relu'\n",
    "))\n",
    "conv_net.add(tf.keras.layers.MaxPooling2D(pool_size = (2,2)))\n",
    "\n",
    "#Segundo bloque\n",
    "conv_net.add(tf.keras.layers.Conv2D(\n",
    "    filters = 16,\n",
    "    kernel_size = 5,\n",
    "    activation = 'relu'\n",
    "))\n",
    "conv_net.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#Tercer bloque\n",
    "conv_net.add(tf.keras.layers.Conv2D(\n",
    "    filters = 32,\n",
    "    kernel_size = 4,\n",
    "    activation = 'relu'\n",
    "))\n",
    "conv_net.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#Cuarto bloque\n",
    "conv_net.add(tf.keras.layers.Conv2D(\n",
    "    filters = 64,\n",
    "    kernel_size = 3,\n",
    "    activation = 'relu'\n",
    "))\n",
    "conv_net.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "#capa flatten, que transforma cualquier arreglo multidimensional en un vector unidimensional.\n",
    "conv_net.add(tf.keras.layers.Flatten())\n",
    "\n",
    "#####Clasificador\n",
    "# Capa densa intermedia\n",
    "conv_net.add(tf.keras.layers.Dense(units=512, activation='relu'))\n",
    "\n",
    "# Capa densa intermedia\n",
    "conv_net.add(tf.keras.layers.Dense(units=512, activation='relu'))\n",
    "\n",
    "# Agregamos dropout para regularización\n",
    "conv_net.add(tf.keras.layers.Dropout(0.2))\n",
    "\n",
    "# Capa de salida\n",
    "conv_net.add(tf.keras.layers.Dense(units=18, #Número de categorías\n",
    "                                   activation='softmax')) #Clasificación multiclase\n",
    "\n",
    "conv_net.summary()\n",
    "tf.keras.utils.plot_model(conv_net,show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_net.compile(loss='categorical_crossentropy',\n",
    "                 optimizer=tf.optimizers.Adam(learning_rate=1e-5),\n",
    "                 metrics = [\"accuracy\"])\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "                  filepath='best_weights_512N.h5',\n",
    "                  monitor=\"val_loss\",\n",
    "                  mode=\"min\",\n",
    "                  save_best_only=True,\n",
    "                  save_weights_only=True\n",
    "              )\n",
    "\n",
    "stopping = tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_loss\",\n",
    "                patience=10,\n",
    "                mode=\"min\",\n",
    "                restore_best_weights=True\n",
    "            )\n",
    "\n",
    "\n",
    "total_samples = X_train.shape[0]\n",
    "validation_samples= X_valid.shape[0]\n",
    "batch_size = 32\n",
    "\n",
    "# Entrenamos el modelo\n",
    "history = conv_net.fit(train_gen_1,\n",
    "                       steps_per_epoch=total_samples // batch_size,\n",
    "                       epochs=50,\n",
    "                       validation_data= valid_gen_1 ,\n",
    "                       #validation_steps=validation_samples // batch_size,\n",
    "                       #verbose=1,\n",
    "                       callbacks=[checkpoint, stopping] )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
